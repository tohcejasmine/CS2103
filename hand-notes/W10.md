# CS2103 Notes (Week 10)

# Lecture (Week 9 - 18/10)

* Law of Demeter
    - Can still call methods of a parameter
    - It's **okay** to create and do stuff to new objects
        - ** It's okay if `new` is not within in the code
        - As long as it is a new object, not objects that have existed already
    - Heavily hinted that could be in the exam paper
* 4 techniques
    1. Misunderstandings
        - Different interpretations
        - Internal/Developer screwed up
        - **Use assertions to abort**
    2. Mishaps
        - User does not do the right things
        - Safety mechanisms
        - **Use exceptions to handle**
        - E.g. API calls Google server but server down
    3. Mysteries
        - **Use logging**
    4. Misuse
        - **Write defensive code**
        - Actively try to block things
        - Identify holes, actively plug them
        - Make sure it never functions incorrectly
* Types of patterns in SE domain
    1. Singleton
        - Context: One and only one object, shared among others
        - Problem: how to avoid multiple objects
        - Solution: 
            - make the constructor private
            - declare variable of the same time
            - static public method to create object if not yet exist
        - Not so good pattern
    2. Facade
        - Context: component with many internal structures, 
        - Problem: Don't want clients to access them directly
        - Solution: put a class that deals with these
            - E.g. `LogicManager`
        - `<<facade>>`,  `<<client>>`, `<<interface>>`, `<<singleton>>`: roles played by the class (don't know actual name)
    3. Command
* Writing test cases
    - How much information to reveal
        - Glass/white-box: look at code, write tests to test all the branches
        - Gray-box: some information is used
            - Design test cases of different conditions
            - E.g. different algo when more than 1000 objects
        - Black-box
    - Cost
        - There is a cost to testing
        - Effectiveness and efficiency of test (E&E)
            - Effective: find more bugs
            - Efficient: use less test cases
        - A test case should have a reasonable change of finding a new bug not found by the other test cases
    - Equivalence partitioning
        - More than 1 partition can produce the same output
        - E.g. within range, outside range
        - Not necessary to test many cases from one partition
            - increases efficiency
        - Cover all partitions
            - increases effectiveness
    - Boundary value analysis
        - Testing at the boundary (minimally), just outside, just inside
        - Depends on how many test cases you can afford
        - Not applicable for cases when values are not continuous
            - E.g. Prime and not prime numbers
        - Improves E&E
            - Cut down redundant test cases
            - Focus on areas where there are likely to be more bugs

-----

# Design Patterns

# Defensive Programming

# Test Case Design: Intro

* Need for deliberate test case design
    - Not practical for _exhaustive testing_ (testing for all possible cases)
    - Often requires a massive/infinite number of test cases
    - Many variations
        - E.g. add to empty list, list with one item, list with _n_ items
        - E.g. add to _English_, _French_, _Spanish_ etc word to list
        - E.g. Add item that is existing in list
        - E.g. add item immediately after adding another item, after system startup 
    - Except for trivial SUTs
* Program testing can be used to show the presence of bugs but never their absence (Dijkstra)
* Every test case adds to the cost of testing
    - E.g. expensive in cases like on-field testing of flight-control software
* Test cases need to be design to make the use of testing resources
* Testing should be _E&E_:
    1. **_Effective_**
        - Finds high pct of existing bugs/number of defects
    2. **_Efficient_**
        - Has a high rate of success (bugs found/test cases)/uses less test cases
* Benefits of _E&E_:
    - Improve quality of SUT
    - Save money
    - Save time spent on test execution
    - Save effort on writing and maintaining tests
    - Minimise redundant test cases
    - Forces us to understand SUT better
- Each new test added should target a potential fault that is not already targeted by existing test cases

* Positive _vs_ Negative Test Cases

| Type | Description |
| --- | --- |
| **Positive test case** | Test is designed to produce an expected/valid behaviour |
| **Negative test case** | Test designed to produce a behaviour that indicates an invalid/unexpected situation (e.g. error message) |

* Black _vs_ Gray _vs_ White-boxes

| Approach | Description|
| --- | --- |
| **Black-box** | AKA _specification-based_/_responsibility-based_ |
| | test cases designed exclusively based on SUT's specified external behaviour |
| **White-box** | AKA _glass-box_/_structured/implementation-based_ |
| | Test cases designed based on what is known about the SUT's implementation (i.e. the code) |
| **Gray-box** | Test case design uses _some_ important info about implementation |
| | E.g. If implementation of a sort operation uses different algorithms to sort lists shorter than 10000 items and lists longer than 10000 items, more meaningful test cases can be added to verify correctness of both algorithms |

* Testing based on Use Cases
    - Use cases used for system testing, acceptance testing
    - E.g. main success scenario one test case, each variation (due to extension) another test case
    - Note: use cases do not specify the exact data entered into system
        - Tester has to choose data by considering equivalence partitions, boundary values
        - Combinations of these, result in one use case forming many test cases
    - Increase _E&E_ of testing
        - Give more attention to high-priority use cases
        - E.g. Scripted approach (i.e. _scripted testing vs exploratory testing_) can be used to test high priority test cases
        - E.g. Exploratory approach used to test other areas of concern that could emerge during testing
    
# Test Case Design: Equivalence Partitioning

# Test Case Design: Boundary Value Analysis (BVA)

* BVA is a test case design heuristic based on the observation that bugs often result from incorrect handling of boundaries of equivalence partitions
* Ends of points of boundaries often used in branching instructions
    - More likely to cause errors
* When picking test inputs from equivalence partition, values near boundaries (i.e. boundary values) are more likely to find bugs
    - Sometimes also called _Corner cases_
* Typically, choose 3 values around boundary to test
    1. One value from boundary
    2. One value just below boundary
    3. One value just above boundary
    - Number of values to pick depends on other factors (e.g cost of each test case)
* Examples:

| Equivalence partition | Some possible boundary values |
| --- | --- |
| `[1-12]` | `0,1,2`,`11,12,13` |
| `[MIN_INT, 0]` where `MIN_INT` is the minimum possible integer value allowed by the environment | `MIN_INT`, `MIN_INT+1`, `-1`, `0` , `1` |
| `[any non-null String]` | `Empty String`, `a String of maximum possible length` |
| `[prime numbers]` | `No specific boundary` |
| `[“F”]` | `No specific boundary` |
| `[“A”, “D”, “X”]` | `No specific boundary` |
| `[non-empty Stack]` (assume fixed size stack) | Stack with `1 element`, `2 elements`, `no empty space`, `only 1 empty space` |